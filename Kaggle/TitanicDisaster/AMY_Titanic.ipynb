{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理和必须的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.3 (default, Mar 27 2019, 22:11:17) \n",
      "[GCC 7.3.0]\n",
      "pandas version: 0.24.2\n",
      "matplotlib version: 3.1.0\n",
      "NumPy version: 1.16.4\n",
      "SciPy version: 1.3.0\n",
      "IPython version: 7.6.1\n",
      "scikit-learn version: 0.21.2\n",
      "-------------------------\n",
      "gender_submission.csv\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "from time import time\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print('-'*25)\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) \n",
    "# will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "# unix系统中每一个进程会返回一个状态码 默认0为进程执行正常,其他为错误码\n",
    "print(check_output([\"ls\", \"./data\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入模型训练的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common Model Algorithms\n",
    "#from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis,gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler,QuantileTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score,cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8\n",
    "# 这个加在这里就不会出现那么多烦人的warning了\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预览数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "#data_raw = pd.read_csv('./data/train.csv')\n",
    "#a dataset should be broken into 3 splits: train, test, and (final) validation\n",
    "#the test file provided is the validation file for competition submission\n",
    "#we will split the train set into train and test data in future sections\n",
    "#X_val  = pd.read_csv('./data/test.csv')\n",
    "# to play with our data we'll create a copy\n",
    "# remember python assignment or equal passes by reference vs values, \n",
    "# so we use the copy function: https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs\n",
    "#X_train = data_raw.copy(deep = True)\n",
    "\n",
    "#however passing by reference is convenient, because we can clean both datasets at once\n",
    "# 训练集和验证集合并在一起\n",
    "#data_cleaner = [X_train, X_val]\n",
    "\n",
    "#preview data\n",
    "#print(data_raw.info()) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\n",
    "#data_raw.head() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n",
    "#data_raw.tail() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\n",
    "#data_raw.sample(10) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成描述统计趋势的描述性统计数据，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print('Train columns with null values:\\n', X_train.isnull().sum())\n",
    "#print(\"-\"*10)\n",
    "\n",
    "#print('Test/Validation columns with null values:\\n', X_val.isnull().sum())\n",
    "#print(\"-\"*10)\n",
    "\n",
    "#data_raw.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完善缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data_raw = pd.read_csv('./data/train.csv')\n",
    "    X_val  = pd.read_csv('./data/test.csv')\n",
    "    X_train = data_raw.copy(deep = True)\n",
    "    data_cleaner = [X_train, X_val]\n",
    "    return data_cleaner\n",
    "\n",
    "def show_data_value_counts_column(column):\n",
    "    data = load_data()\n",
    "    X_train= data[0]\n",
    "    X_val = data[1]\n",
    "    print('Test/Validation value counts:\\n', X_train[column].value_counts())\n",
    "    print(\"-\"*10)\n",
    "    print('Test/Validation value counts:\\n', X_val[column].value_counts())\n",
    "    print(\"-\"*10)\n",
    "    \n",
    "def concat_df(X_train, X_val):\n",
    "    # Returns a concatenated df of training and test set on axis 0\n",
    "    return pd.concat([X_train, X_val], sort=True).reset_index(drop=True)\n",
    "\n",
    "def divide_df(X):\n",
    "    # Returns divided dfs of training and test set\n",
    "    X_train = X.loc[:890]\n",
    "    X_val = X.loc[891:].drop(['Survived'],axis=1)\n",
    "    return X_train,X_val\n",
    "\n",
    "def load_all_data():\n",
    "    X_train, X_val = load_data()\n",
    "    return concat_df(X_train,X_val)\n",
    "\n",
    "def show_data_na():\n",
    "    data = load_data()\n",
    "    X_train= data[0]\n",
    "    X_val = data[1]\n",
    "    print('Train columns with null values:\\n', X_train.isnull().sum())\n",
    "    print(\"-\"*10)\n",
    "\n",
    "    print('Test/Validation columns with null values:\\n', X_val.isnull().sum())\n",
    "    print(\"-\"*10)\n",
    "    \n",
    "def show_data_info():\n",
    "    data = load_data()\n",
    "    X_train= data[0]\n",
    "    X_val = data[1]\n",
    "    print('Train columns info:\\n', X_train.info())\n",
    "    print(\"-\"*10)\n",
    "    print('Test/Validation info:\\n', X_val.info())\n",
    "    print(\"-\"*10)   \n",
    "    \n",
    "def show_data_describe_column(column):\n",
    "    data = load_data()\n",
    "    X_train= data[0]\n",
    "    X_val = data[1]\n",
    "    print('Train columns describe:\\n', X_train[column].describe())\n",
    "    print('Test/Validation value counts:\\n', X_train[column].value_counts())\n",
    "    print(\"-\"*10)\n",
    "    print('Test/Validation describe:\\n', X_val[column].describe())\n",
    "    print('Test/Validation value counts:\\n', X_val[column].value_counts())\n",
    "    print(\"-\"*10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 12 columns):\n",
      "Age            1046 non-null float64\n",
      "Cabin          295 non-null object\n",
      "Embarked       1307 non-null object\n",
      "Fare           1308 non-null float64\n",
      "Name           1309 non-null object\n",
      "Parch          1309 non-null int64\n",
      "PassengerId    1309 non-null int64\n",
      "Pclass         1309 non-null int64\n",
      "Sex            1309 non-null object\n",
      "SibSp          1309 non-null int64\n",
      "Survived       891 non-null float64\n",
      "Ticket         1309 non-null object\n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 122.8+ KB\n"
     ]
    }
   ],
   "source": [
    "load_all_data().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过分析相关系数,用groupby通过Sex和Pclass分级对Age求平均值进行缺失值填充.\n",
    "def completing_age_na(dt):\n",
    "    for X in dt:\n",
    "        X['Age'] = X.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用最多上岸港口进行填充\n",
    "def completing_embarked_na(dt):\n",
    "    for X in dt:\n",
    "        X['Embarked'].fillna('S',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare 只缺失一个值,通过相关系数分析,Fare和Parch,SibSp,Pclass 相关程度最高.\n",
    "def completing_fare_na(dt):\n",
    "    for X in dt:\n",
    "        med_fare = X.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n",
    "        X['Fare'].fillna(med_fare, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completing_cabin_na(dt,shrink):\n",
    "    for X in dt:\n",
    "        X['Deck'] = X['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
    "        # T只有一个乘客,归为A类\n",
    "        idx = X[X['Deck'] == 'T'].index\n",
    "        X.loc[idx, 'Deck'] = 'A'\n",
    "        # 缩减特征维度\n",
    "        if shrink :\n",
    "            X['Deck'] = X['Deck'].replace(['A', 'B', 'C'], 'ABC')\n",
    "            X['Deck'] = X['Deck'].replace(['D', 'E'], 'DE')\n",
    "            X['Deck'] = X['Deck'].replace(['F', 'G'], 'FG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def drop_useless(dt,drop_column):\n",
    "    for X in dt:\n",
    "        X.drop(columns=drop_column, axis=1, inplace = True)\n",
    "        X.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建新的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 家庭大小.\n",
    "def create_familysize(dt,shrink):\n",
    "    for X in dt:\n",
    "        X['FamilySize'] = X ['SibSp'] + X['Parch'] + 1\n",
    "        # 高维特征\n",
    "        if shrink:\n",
    "            family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\n",
    "            X['FamilySizeGrouped'] = X['FamilySize'].map(family_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 许多乘客与团体一起旅行。这些团体由朋友，保姆，女佣等组成。他们不算家庭，但他们使用相同的票。\n",
    "def create_ticket_freq(dt):\n",
    "    for X in dt:\n",
    "        X['TicketFrequency'] = X.groupby('Ticket')['Ticket'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 票价分布 用频率划分\n",
    "def create_FareBin(dt,bins = 13):\n",
    "    for X in dt:\n",
    "        X['FareBin'] = pd.qcut(X['Fare'], q = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 年龄 同样用频率划分\n",
    "def create_AgeBin(dt,bins = 10):\n",
    "    for X in dt:\n",
    "        X['AgeBin'] = pd.qcut(X['Age'].astype(int), q = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 头衔\n",
    "def create_title(dt,shrink):\n",
    "    for X in dt:\n",
    "        X['Title'] = X['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n",
    "        if shrink :\n",
    "            X['IsMarried'] = 0\n",
    "            X['IsMarried'].loc[X['Title'] == 'Mrs'] = 1\n",
    "            X['Title'] = X['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n",
    "            X['Title'] = X['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don'], 'Dr/Military/Noble/Clergy')\n",
    "            # Leave Master and Rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取每个乘客的姓氏,\n",
    "def create_familyonboard(dt):\n",
    "    def extract_surname(X):    \n",
    "        families = []\n",
    "        for i in range(len(X)):        \n",
    "            name = X.iloc[i]\n",
    "            if '(' in name:\n",
    "                name_no_bracket = name.split('(')[0] \n",
    "            else:\n",
    "                name_no_bracket = name\n",
    "            family = name_no_bracket.split(',')[0]\n",
    "            title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n",
    "            for c in string.punctuation:\n",
    "                family = family.replace(c, '').strip()\n",
    "            families.append(family)\n",
    "        return families\n",
    "    for X in dt:\n",
    "        X['FamilyOnBoard'] = extract_surname(X['Name'])\n",
    "    #X['FamilyOnBoardSize'] = X.groupby('FamilyOnBoard')['FamilyOnBoard'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 魔法特征.\n",
    "def create_survival_rate(dt, shrink=True):\n",
    "    X_train,X_val = dt[0],dt[1]\n",
    "\n",
    "    # 创建票号或者家庭名称同时出现在验证集和训练集中的列表.\n",
    "    non_unique_families = [x for x in X_train['FamilyOnBoard'].unique() \\\n",
    "                               if x in X_val['FamilyOnBoard'].unique()]\n",
    "    non_unique_tickets = [x for x in X_train['Ticket'].unique() \\\n",
    "                              if x in X_val['Ticket'].unique()]\n",
    "\n",
    "    # 多用于可视化\n",
    "    df_family_survival_rate = X_train.groupby('FamilyOnBoard')['Survived', 'FamilyOnBoard','FamilySize'].median()\n",
    "    df_ticket_survival_rate = X_train.groupby('Ticket')['Survived', 'Ticket','TicketFrequency'].median()\n",
    "\n",
    "    family_rates = {}\n",
    "    ticket_rates = {}\n",
    "\n",
    "    # 搜集同时在训练集和验证集中家庭数目 > 1 的生存率,index是family的名称.1是FamilySize,0是生存率\n",
    "    for i in range(len(df_family_survival_rate)):\n",
    "        if df_family_survival_rate.index[i] in non_unique_families and \\\n",
    "            df_family_survival_rate.iloc[i, 1] > 1:\n",
    "            family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n",
    "\n",
    "    # 搜集同时在训练集和验证集中票出现频率 > 1 的生存率,index是Ticket的名称.1是TicketFrequency,0是生存率\n",
    "    for i in range(len(df_ticket_survival_rate)):\n",
    "        if df_ticket_survival_rate.index[i] in non_unique_tickets and \\\n",
    "            df_ticket_survival_rate.iloc[i, 1] > 1:\n",
    "            ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]\n",
    "\n",
    "\n",
    "    # 平均生存率\n",
    "    mean_survival_rate = np.mean(X_train['Survived'])\n",
    "\n",
    "    # 训练集和验证集的家庭所对应的生存率\n",
    "    train_family_survival_rate = np.empty(X_train.shape[0])\n",
    "    val_family_survival_rate = np.empty(X_val.shape[0])\n",
    "    # 辅助特征*_family_survival_rate_NA,0或1分别代表同时没出现/出现在测试集和验证集中,没出现的赋予平均生存率\n",
    "    train_family_survival_rate_NA = np.empty(X_train.shape[0],dtype='int32')\n",
    "    val_family_survival_rate_NA = np.empty(X_val.shape[0],dtype='int32')\n",
    "\n",
    "    # 训练集\n",
    "    for i in range(len(X_train)):\n",
    "        familyOnBoard = X_train['FamilyOnBoard'].iloc[i]\n",
    "        if familyOnBoard in family_rates:\n",
    "            train_family_survival_rate[i] = family_rates[familyOnBoard]\n",
    "            train_family_survival_rate_NA[i] = 1\n",
    "        else:\n",
    "            train_family_survival_rate[i] = mean_survival_rate\n",
    "            train_family_survival_rate_NA[i] = 0\n",
    "\n",
    "    # 验证集\n",
    "    for i in range(len(X_val)):\n",
    "        familyOnBoard = X_val['FamilyOnBoard'].iloc[i]\n",
    "        if familyOnBoard in family_rates:\n",
    "            val_family_survival_rate[i] = family_rates[familyOnBoard]\n",
    "            val_family_survival_rate_NA[i] = 1\n",
    "        else:\n",
    "            val_family_survival_rate[i] = mean_survival_rate\n",
    "            val_family_survival_rate_NA[i] = 0\n",
    "\n",
    "    X_train['FamilySurvivalRate'] = train_family_survival_rate\n",
    "    X_train['FamilySurvivalRateNA'] = train_family_survival_rate_NA\n",
    "    X_val['FamilySurvivalRate'] = val_family_survival_rate\n",
    "    X_val['FamilySurvivalRateNA'] = val_family_survival_rate_NA\n",
    "\n",
    "    # 训练集和验证集的票号所对应的生存率\n",
    "    train_ticket_survival_rate = np.empty(X_train.shape[0])\n",
    "    val_ticket_survival_rate = np.empty(X_val.shape[0])\n",
    "    # 辅助特征*_ticket_survival_rate_NA,0或1分别代表同时没出现/出现在测试集和验证集中,没出现的赋予平均生存率\n",
    "    train_ticket_survival_rate_NA = np.empty(X_train.shape[0],dtype='int32')\n",
    "    val_ticket_survival_rate_NA = np.empty(X_val.shape[0],dtype='int32')\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        ticket = X_train['Ticket'].iloc[i]\n",
    "        if ticket in ticket_rates:\n",
    "            train_ticket_survival_rate[i] = ticket_rates[ticket]\n",
    "            train_ticket_survival_rate_NA[i] = 1\n",
    "        else:\n",
    "            train_ticket_survival_rate[i] = mean_survival_rate\n",
    "            train_ticket_survival_rate_NA[i] = 0\n",
    "\n",
    "    for i in range(len(X_val)):\n",
    "        ticket = X_val['Ticket'].iloc[i]\n",
    "        if ticket in ticket_rates:\n",
    "            val_ticket_survival_rate[i] = ticket_rates[ticket]\n",
    "            val_ticket_survival_rate_NA[i] = 1\n",
    "        else:\n",
    "            val_ticket_survival_rate[i] = mean_survival_rate\n",
    "            val_ticket_survival_rate_NA[i] = 0\n",
    "\n",
    "    X_train['TicketSurvivalRate'] = train_ticket_survival_rate\n",
    "    X_train['TicketSurvivalRateNA'] = train_ticket_survival_rate_NA\n",
    "    X_val['TicketSurvivalRate'] = val_ticket_survival_rate\n",
    "    X_val['TicketSurvivalRateNA'] = val_ticket_survival_rate_NA\n",
    "\n",
    "    if shrink:\n",
    "        for X_sub in [X_train, X_val]:\n",
    "            X_sub['SurvivalRate'] = (X_sub['TicketSurvivalRate'] + X_sub['FamilySurvivalRate']) / 2\n",
    "            X_sub['SurvivalRateNA'] = (X_sub['TicketSurvivalRateNA'] + X_sub['FamilySurvivalRateNA']) / 2  \n",
    "    \n",
    "    dt[0]=X_train\n",
    "    dt[1]=X_val\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的转换,离散化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_X(dt):\n",
    "    for X in dt:\n",
    "        X['AgeBin'] = LabelEncoder().fit_transform(X['AgeBin'])\n",
    "        X['FareBin'] = LabelEncoder().fit_transform(X['FareBin'])\n",
    "        X['Deck'] = LabelEncoder().fit_transform(X['Deck'])\n",
    "        X['Title'] = LabelEncoder().fit_transform(X['Title'])\n",
    "        X['Sex'] = LabelEncoder().fit_transform(X['Sex'])\n",
    "        X['Embarked'] = LabelEncoder().fit_transform(X['Embarked'])\n",
    "        X['FamilySizeGrouped'] = LabelEncoder().fit_transform(X['FamilySizeGrouped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarized(dt,columns):\n",
    "    for X in dt:\n",
    "        for c in columns:\n",
    "            X_c = X[c]\n",
    "            X_c = StandardScaler().fit_transform(np.array(X_c).reshape(-1,1))\n",
    "            X[c] = X_c.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data():\n",
    "    dt = load_data() # 综合在一起处理\n",
    "    # ------------------ 缺失值处理 ------------------\n",
    "    # 处理年龄缺失值\n",
    "    completing_age_na(dt)\n",
    "    # 处理cabin缺失值\n",
    "    completing_cabin_na(dt,True)\n",
    "    # 处理embarked缺失值\n",
    "    completing_embarked_na(dt)\n",
    "    # 处理fare缺失值\n",
    "    completing_fare_na(dt)\n",
    "    # ------------------ 创建新的特征 --------------------\n",
    "    # 家庭成员多少\n",
    "    create_familysize(dt,True)\n",
    "    # 票频率\n",
    "    create_ticket_freq(dt)\n",
    "    # 年龄分段\n",
    "    create_AgeBin(dt)\n",
    "    # 票价分段\n",
    "    create_FareBin(dt)\n",
    "    # 头衔\n",
    "    create_title(dt,True)\n",
    "    # 家族名称\n",
    "    create_familyonboard(dt)\n",
    "    # 生存率\n",
    "    dt = create_survival_rate(dt, True)\n",
    "    # ------------------ 数据离散化 归一化 --------------------\n",
    "    # 离散化数据\n",
    "    convert_X(dt)\n",
    "    \n",
    "    # 丢弃无用的列\n",
    "    drop_useless(\n",
    "        dt \n",
    "        ,[\n",
    "          'PassengerId'\n",
    "        , 'Cabin'\n",
    "        , 'Ticket'\n",
    "        , 'Name'\n",
    "        , 'Fare'\n",
    "        #, 'Sex' \n",
    "        , 'FamilySize'\n",
    "        , 'FamilyOnBoard'\n",
    "        , 'Age'\n",
    "        #, 'Title'\n",
    "        #, 'Embarked'\n",
    "        , 'SibSp'\n",
    "        , 'Parch'\n",
    "        , 'SurvivalRate'\n",
    "        , 'SurvivalRateNA'\n",
    "        , 'FamilySurvivalRate'\n",
    "        , 'FamilySurvivalRateNA'\n",
    "        #, 'TicketSurvivalRate'\n",
    "        #, 'TicketSurvivalRateNA'\n",
    "    ])\n",
    "    \n",
    "    # 升维\n",
    "    dummy_columns = [\n",
    "        'Embarked'\n",
    "        ,'Sex'\n",
    "        ,'Title'\n",
    "        ,'Pclass'\n",
    "        ,'Deck'\n",
    "        #,'FamilySize'\n",
    "        #,'TicketFrequency'\n",
    "        ,'FamilySizeGrouped'\n",
    "        #,'AgeBin'\n",
    "        #,'FareBin'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    #standarized(X,columns =['AgeBin','FareBin','TicketFrequency'])\n",
    "    #X = pd.DataFrame(X)\n",
    "    X_train,X_val=dt[0],dt[1]\n",
    "    X_train = pd.get_dummies(X_train,columns=dummy_columns)\n",
    "    X_val = pd.get_dummies(X_val,columns=dummy_columns)\n",
    "    y_train = X_train['Survived']\n",
    "    X_train.drop(columns=['Survived'], axis = 1, inplace = True)\n",
    "    \n",
    "    #X_train = StandardScaler().fit_transform(X_train)\n",
    "    #X_val = StandardScaler().fit_transform(X_val)\n",
    "    \n",
    "    \n",
    "    return X_train , y_train , X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TicketFrequency</th>\n",
       "      <th>AgeBin</th>\n",
       "      <th>FareBin</th>\n",
       "      <th>IsMarried</th>\n",
       "      <th>TicketSurvivalRate</th>\n",
       "      <th>TicketSurvivalRateNA</th>\n",
       "      <th>Embarked_0</th>\n",
       "      <th>Embarked_1</th>\n",
       "      <th>Embarked_2</th>\n",
       "      <th>Sex_0</th>\n",
       "      <th>...</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Deck_0</th>\n",
       "      <th>Deck_1</th>\n",
       "      <th>Deck_2</th>\n",
       "      <th>Deck_3</th>\n",
       "      <th>FamilySizeGrouped_0</th>\n",
       "      <th>FamilySizeGrouped_1</th>\n",
       "      <th>FamilySizeGrouped_2</th>\n",
       "      <th>FamilySizeGrouped_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TicketFrequency  AgeBin  FareBin  IsMarried  TicketSurvivalRate  \\\n",
       "185                1       7       10          0            0.383838   \n",
       "828                1       3        1          0            0.383838   \n",
       "395                1       2        2          0            0.383838   \n",
       "88                 4       3       12          0            0.500000   \n",
       "567                4       5        7          1            0.000000   \n",
       "\n",
       "     TicketSurvivalRateNA  Embarked_0  Embarked_1  Embarked_2  Sex_0  ...  \\\n",
       "185                     0           0           0           1      0  ...   \n",
       "828                     0           0           1           0      0  ...   \n",
       "395                     0           0           0           1      0  ...   \n",
       "88                      1           0           0           1      1  ...   \n",
       "567                     1           0           0           1      1  ...   \n",
       "\n",
       "     Pclass_2  Pclass_3  Deck_0  Deck_1  Deck_2  Deck_3  FamilySizeGrouped_0  \\\n",
       "185         0         0       1       0       0       0                    1   \n",
       "828         0         1       0       0       0       1                    1   \n",
       "395         0         1       0       0       0       1                    1   \n",
       "88          0         0       1       0       0       0                    0   \n",
       "567         0         1       0       0       0       1                    0   \n",
       "\n",
       "     FamilySizeGrouped_1  FamilySizeGrouped_2  FamilySizeGrouped_3  \n",
       "185                    0                    0                    0  \n",
       "828                    0                    0                    0  \n",
       "395                    0                    0                    0  \n",
       "88                     0                    1                    0  \n",
       "567                    0                    1                    0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train ,y_train, X_val = get_clean_data()\n",
    "X_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TicketFrequency', 'AgeBin', 'FareBin', 'IsMarried',\n",
       "       'TicketSurvivalRate', 'TicketSurvivalRateNA', 'Embarked_0',\n",
       "       'Embarked_1', 'Embarked_2', 'Sex_0', 'Sex_1', 'Title_0', 'Title_1',\n",
       "       'Title_2', 'Title_3', 'Title_4', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n",
       "       'Deck_0', 'Deck_1', 'Deck_2', 'Deck_3', 'FamilySizeGrouped_0',\n",
       "       'FamilySizeGrouped_1', 'FamilySizeGrouped_2', 'FamilySizeGrouped_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TicketFrequency', 'AgeBin', 'FareBin', 'IsMarried',\n",
       "       'TicketSurvivalRate', 'TicketSurvivalRateNA', 'Embarked_0',\n",
       "       'Embarked_1', 'Embarked_2', 'Sex_0', 'Sex_1', 'Title_0', 'Title_1',\n",
       "       'Title_2', 'Title_3', 'Title_4', 'Pclass_1', 'Pclass_2', 'Pclass_3',\n",
       "       'Deck_0', 'Deck_1', 'Deck_2', 'Deck_3', 'FamilySizeGrouped_0',\n",
       "       'FamilySizeGrouped_1', 'FamilySizeGrouped_2', 'FamilySizeGrouped_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 27 columns):\n",
      "TicketFrequency         891 non-null int64\n",
      "AgeBin                  891 non-null int64\n",
      "FareBin                 891 non-null int64\n",
      "IsMarried               891 non-null int64\n",
      "TicketSurvivalRate      891 non-null float64\n",
      "TicketSurvivalRateNA    891 non-null int32\n",
      "Embarked_0              891 non-null uint8\n",
      "Embarked_1              891 non-null uint8\n",
      "Embarked_2              891 non-null uint8\n",
      "Sex_0                   891 non-null uint8\n",
      "Sex_1                   891 non-null uint8\n",
      "Title_0                 891 non-null uint8\n",
      "Title_1                 891 non-null uint8\n",
      "Title_2                 891 non-null uint8\n",
      "Title_3                 891 non-null uint8\n",
      "Title_4                 891 non-null uint8\n",
      "Pclass_1                891 non-null uint8\n",
      "Pclass_2                891 non-null uint8\n",
      "Pclass_3                891 non-null uint8\n",
      "Deck_0                  891 non-null uint8\n",
      "Deck_1                  891 non-null uint8\n",
      "Deck_2                  891 non-null uint8\n",
      "Deck_3                  891 non-null uint8\n",
      "FamilySizeGrouped_0     891 non-null uint8\n",
      "FamilySizeGrouped_1     891 non-null uint8\n",
      "FamilySizeGrouped_2     891 non-null uint8\n",
      "FamilySizeGrouped_3     891 non-null uint8\n",
      "dtypes: float64(1), int32(1), int64(4), uint8(21)\n",
      "memory usage: 56.6 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve_1(\n",
    "    estimator\n",
    "    ,title\n",
    "    , X\n",
    "    , y\n",
    "    ,ax=None #选择子图\n",
    "    ,ylim=None #设置纵坐标的取值范围\n",
    "    ,cv=None #交叉验证\n",
    "    ,n_jobs=-1 #设定索要使用的线程\n",
    "    ):\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator\n",
    "        , X\n",
    "        , y\n",
    "        ,shuffle=True\n",
    "        ,cv=cv\n",
    "        ,n_jobs=n_jobs\n",
    "    )\n",
    "    if ax == None:\n",
    "        ax = plt.gca()\n",
    "    else:\n",
    "        ax = plt.figure()\n",
    "    ax.set_title(title)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "    ax.set_xlabel(\"Training examples\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.grid() #绘制网格，不是必须\n",
    "    ax.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', color=\"r\",label=\"Training score\")\n",
    "    ax.plot(train_sizes, np.mean(test_scores, axis=1), 'o-'\n",
    "    , color=\"g\",label=\"Test score\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    print('overfiting : ' ,(np.mean(train_scores, axis=1)[-1] - np.mean(test_scores, axis=1)[-1]))\n",
    "    print(np.mean(test_scores, axis=1)[-1])\n",
    "    return ax\n",
    "\n",
    "def plot_learning_curve(clf,X_train,y_train,cv_s=10):\n",
    "    cv = KFold(n_splits=cv_s, shuffle = True)\n",
    "    plot_learning_curve_1(\n",
    "        clf\n",
    "        ,'clf'\n",
    "        ,X_train\n",
    "        ,y_train\n",
    "        ,cv=cv\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xgbcv_learning_curve(params,dtrain,T=100,cv=10,metrics=['error']):\n",
    "    trees = np.arange(0,T,1)\n",
    "    cv_res = xgb.cv(params=params,dtrain=dtrain,num_boost_round=T,nfold=10,metrics=metrics)\n",
    "    train_err = cv_res.iloc[:,0]\n",
    "    test_err = cv_res.iloc[:,2]\n",
    "    gap = test_err - train_err\n",
    "    \n",
    "    #print('min err train arg: train :',np.argmin(train_err) ,' test : ',np.argmin(test_err))\n",
    "    #print('min err train train :',np.min(train_err) ,' test : ',np.min(test_err))\n",
    "\n",
    "    print('min err gap :', np.min(np.abs(train_err - test_err)))\n",
    "    print('last err gap',test_err.iloc[-1]-train_err.iloc[-1])\n",
    "\n",
    "    argmin = np.argmin(np.abs(train_err - test_err))\n",
    "    print('min err gap arg:', argmin)\n",
    "    print('min err gap arg:', train_err[argmin] , ' ||| ' ,test_err[argmin])\n",
    "    \n",
    "    print('last err train : ',train_err.iloc[-1] ,' test : ',test_err.iloc[-1])\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(trees,test_err-train_err,label='err gap')\n",
    "    plt.plot(trees,train_err,label='train err')\n",
    "    plt.plot(trees,test_err,label='test err')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nT=10\\ndtrain = xgb.DMatrix(X_train,y_train)\\ndtest = xgb.DMatrix(X_val)\\nmodel = xgb.train(params,dtrain,num_boost_round=T)\\nres = np.zeros(len(X_val),dtype=\\'int64\\')\\nres[model.predict(dtest)>=0.5]=1\\n\\n_,X_res = load_data()\\nX_res[\\'Survived\\'] = res\\nsubmit = X_res[[\\'PassengerId\\',\\'Survived\\']]\\nsubmit.to_csv(\"./result/submit_amy.csv\", index=False)\\n\\nprint(\\'Validation Data Distribution: \\n\\', X_res[\\'Survived\\'].value_counts(normalize = True))\\nsubmit.sample(10)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "params={\n",
    "    'gamma' : 3\n",
    "    ,'max_depth':20\n",
    "    #, 'eta': 0.2\n",
    "    #, 'min_child_weight': 4\n",
    "    #, 'alpha':5\n",
    "    #, 'lambda':3\n",
    "    #, 'colsample_bylevel':0.75\n",
    "    #,'objective':'binary:logistic'\n",
    "}\n",
    "T=20\n",
    "dtrain = xgb.DMatrix(X_train,y_train)\n",
    "plot_xgbcv_learning_curve(params,dtrain,T=T)\n",
    "model = xgb.train(params,dtrain,num_boost_round=T)\n",
    "'''\n",
    "'''\n",
    "xgb.plot_importance(model)\n",
    "'''\n",
    "'''\n",
    "T=10\n",
    "dtrain = xgb.DMatrix(X_train,y_train)\n",
    "dtest = xgb.DMatrix(X_val)\n",
    "model = xgb.train(params,dtrain,num_boost_round=T)\n",
    "res = np.zeros(len(X_val),dtype='int64')\n",
    "res[model.predict(dtest)>=0.5]=1\n",
    "\n",
    "_,X_res = load_data()\n",
    "X_res['Survived'] = res\n",
    "submit = X_res[['PassengerId','Survived']]\n",
    "submit.to_csv(\"./result/submit_amy.csv\", index=False)\n",
    "\n",
    "print('Validation Data Distribution: \\n', X_res['Survived'].value_counts(normalize = True))\n",
    "submit.sample(10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 39150 candidates, totalling 195750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   60.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 14434 tasks      | elapsed: 18.2min\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed: 20.5min\n",
      "[Parallel(n_jobs=-1)]: Done 18034 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=-1)]: Done 19984 tasks      | elapsed: 25.0min\n",
      "[Parallel(n_jobs=-1)]: Done 22034 tasks      | elapsed: 27.5min\n",
      "[Parallel(n_jobs=-1)]: Done 24184 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=-1)]: Done 26434 tasks      | elapsed: 32.6min\n",
      "[Parallel(n_jobs=-1)]: Done 28784 tasks      | elapsed: 35.7min\n",
      "[Parallel(n_jobs=-1)]: Done 31234 tasks      | elapsed: 38.7min\n",
      "[Parallel(n_jobs=-1)]: Done 33784 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=-1)]: Done 36434 tasks      | elapsed: 45.2min\n",
      "[Parallel(n_jobs=-1)]: Done 39184 tasks      | elapsed: 48.7min\n",
      "[Parallel(n_jobs=-1)]: Done 42034 tasks      | elapsed: 52.4min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-bf9fcf7aef03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'min_samples_leaf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/A/develop/dt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/A/develop/dt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/A/develop/dt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/A/develop/dt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/A/develop/dt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/A/develop/dt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/A/develop/dt/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/A/develop/dt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid={\n",
    "    'n_estimators':np.arange(100,1000,100),\n",
    "    'max_depth':np.arange(1,6,1),\n",
    "    'min_samples_split':np.linspace(0.01,1.,30),\n",
    "    'min_samples_leaf':np.arange(1,30,1)\n",
    "}\n",
    "gcv = GridSearchCV(RandomForestClassifier(),param_grid=param_grid,verbose=True,cv=5,n_jobs=-1).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100\n",
    "    ,max_depth=3\n",
    "    #,min_samples_split=15\n",
    "    #,min_samples_leaf=7\n",
    "    #,random_state=5\n",
    ")\n",
    "plot_learning_curve(rf,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(rf, \"rf_model.m\")\n",
    "plot_learning_curve(rf,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_estimator(X_train,y_train):\n",
    "    train_scores = []\n",
    "    test_scores=[]\n",
    "    #for n_estimators in np.arange(50, 200, 50):\n",
    "    n_estimators=400\n",
    "    for max_depth in np.arange(1,20,1):\n",
    "        rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,n_jobs=-1)\n",
    "        cv_score = cross_validate(rf,X_train,y_train,return_train_score=True)\n",
    "        test_scores.append(cv_score['test_score'].mean())\n",
    "        train_scores.append(cv_score['train_score'].mean())\n",
    "    return train_scores,test_scores\n",
    "\n",
    "train_scores,test_scores = show_estimator(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps = np.array(train_scores) - np.array(test_scores)\n",
    "indexes=range(len(train_scores))\n",
    "tr=train_scores[:10]\n",
    "te=test_scores[:10]\n",
    "ga=gaps[:10]\n",
    "ind=indexes[:10]\n",
    "plt.plot(ind,tr)\n",
    "plt.plot(ind,te)\n",
    "plt.plot(ind,ga)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train,y_train)\n",
    "res = rf.predict(X_val)\n",
    "_,X_res=  load_data()\n",
    "X_res['Survived'] = res\n",
    "submit = X_res[['PassengerId','Survived']]\n",
    "submit.to_csv(\"./result/submit_amy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit = pd.read_csv('./CheatingData/titanic.csv')\n",
    "tit['survived'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
