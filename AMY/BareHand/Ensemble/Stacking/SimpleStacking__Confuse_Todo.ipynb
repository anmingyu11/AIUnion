{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "# 基学习器\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# 次级学习器\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# 预处理\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "#from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking先从初始数据集训练出初级学习器,然后'生成'一个新数据集用于训练次级学习器,在这个新数据集中,初级学习器的输出被当做样例输入特征,而初始样本标记仍被当做样例标记.\n",
    "\n",
    "初级学习器是异质的.\n",
    "\n",
    "在训练阶段,次级训练集是利用初级学习器产生的,若直接用初级学习器的训练集来产生次级训练集,则过拟合风险会比较大;因此,一般是通过使用交叉验证或留一法这样的方式,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = load_digits()\n",
    "    return data.data,data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_split():\n",
    "    data = load_digits()\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ before ------------------------------\n",
      "0.0\n",
      "16.0\n",
      "------------------------------ after ------------------------------\n",
      "-3.018816281385554\n",
      "36.68787265568844\n"
     ]
    }
   ],
   "source": [
    "X,y = load_data()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "def standardize(X_):\n",
    "    X_ = StandardScaler().fit_transform(X_)\n",
    "    return X_\n",
    "\n",
    "print('-'*30 ,'before','-'*30)\n",
    "print(X_train.min())\n",
    "print(X_train.max())\n",
    "X_train = standardize(X_train)\n",
    "X_test = standardize(X_test)\n",
    "print('-'*30 ,'after','-'*30)\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "16.0\n",
      "-2.9612382936006596\n",
      "36.68787265568888\n"
     ]
    }
   ],
   "source": [
    "def load_data_split_standarlize():\n",
    "    X_train,X_test,y_train,y_test = load_data_split()\n",
    "    return standardize(X_train),standardize(X_test),y_train,y_test\n",
    "X_1,X_2,_,_=load_data_split()\n",
    "print(X_1.min())\n",
    "print(X_2.max())\n",
    "X_train,X_test,y_train,y_test = load_data_split_standarlize()\n",
    "print(X_train.min())\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_rand_seed():\n",
    "    return np.random.randint(2**32 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "        15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "        12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "         0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "        10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,\n",
       "         9.,  0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7.,\n",
       "        15., 16., 16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,\n",
       "         0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16.,\n",
       "        16.,  6.,  0.,  0.,  0.,  0.,  0., 11., 16., 10.,  0.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y = load_data()\n",
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_indexes(X,y,n_fold=5):\n",
    "    kf = KFold(shuffle=True,n_splits=n_fold)\n",
    "#   return [(train_index.shape,test_index.shape) for train_index,test_index in kf.split(X,y)] \n",
    "    return [(train_index,test_index) for train_index,test_index in kf.split(X,y)] \n",
    "# kfold_indexes(X,y)\n",
    "#X,y = load_data()\n",
    "#kf_indexes = kfold_indexes(X,y)\n",
    "\n",
    "#hole_i = np.array([],dtype=np.int32)\n",
    "#for i,kf_sub in enumerate(kf_indexes):\n",
    "#    hole_i = np.hstack([hole_i,kf_sub[1]])\n",
    "#print(len(np.unique(hole_i)))\n",
    "#print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_test,y_predict):\n",
    "    return np.sum(y_predict == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack1 mean scores :  0.7586666666666667\n",
      "stack1 scores:  {'SVC': 0.9866666666666667, 'LogisticRegression': 0.9533333333333334, 'GaussianNB': 0.1111111111111111, 'MLPClassifier': 0.9777777777777777, 'DecisionTreeClassifier': 0.7644444444444445}\n",
      "-------------------- Stacking score --------------------\n",
      "0.9755555555555555\n",
      "-------------------- stack1_scores --------------------\n",
      "0.9822222222222222\n",
      "0.9555555555555556\n",
      "0.1111111111111111\n",
      "0.9711111111111111\n",
      "0.7577777777777778\n"
     ]
    }
   ],
   "source": [
    "class SimpleStacking :#(BaseEstimator, RegressorMixin, TransformerMixin): sklearn中的超类.\n",
    "\n",
    "    def __init__(\n",
    "        self\n",
    "        , models_stack_1 # 初级学习算法\n",
    "        , models_stack_2 # 次级学习算法\n",
    "        , n_fold = 6\n",
    "    ):\n",
    "        self.models_stack_1_ = models_stack_1\n",
    "        self.models_stack_2_ = models_stack_2\n",
    "        self.T_ = len(self.models_stack_1_)\n",
    "        self.n_fold_ = n_fold\n",
    "        self.X_2_train_ = None\n",
    "        self.X_2_test_ = None\n",
    "\n",
    "    def __kfold_indexes(\n",
    "        self\n",
    "        , X\n",
    "        ,y\n",
    "    ):\n",
    "        '''\n",
    "        K折交叉验证分成 self.n_folds_ 个集合\n",
    "        '''\n",
    "        kf = KFold(\n",
    "            shuffle=True\n",
    "            ,n_splits=self.n_fold_\n",
    "        )\n",
    "        return [(train_index,test_index) for train_index,test_index in kf.split(X,y)] \n",
    "    \n",
    "    def train_stack_1(\n",
    "        self\n",
    "        ,X\n",
    "        ,y\n",
    "    ):\n",
    "        '''\n",
    "        训练初级学习器\n",
    "        @return 初级学习器的学习结果\n",
    "        '''\n",
    "        n = len(y)\n",
    "        # 避免过拟合,将训练集分为k折\n",
    "        kf_indexes = self.__kfold_indexes(X ,y)\n",
    "        # 生成的新的训练集X_2.shape = (n,T)\n",
    "        X_2 = np.empty(shape=(n, self.T_) ,dtype=np.int32)\n",
    "        # 训练初级学习器\n",
    "        models = self.models_stack_1_\n",
    "        for t in range(self.T_):\n",
    "            # 第t个学习器的训练结果\n",
    "            y_res_t = np.empty(shape=n, dtype=np.int32)\n",
    "            # 第j折的数据\n",
    "            for kf_j in kf_indexes:\n",
    "                # 第j折数据的训练集和测试集的索引\n",
    "                kf_j_train,kf_j_test = kf_j[0],kf_j[1]\n",
    "                X_j_train,X_j_test,y_j_train,y_j_test = X[kf_j_train],X[kf_j_test],y[kf_j_train],y[kf_j_test]\n",
    "                models[t].fit(X_j_train,y_j_train)\n",
    "                y_t_p = models[t].predict(X_j_test)\n",
    "                # X_2对应新的特征t的值\n",
    "                y_res_t[kf_j_test] = y_t_p\n",
    "            # 给特征t即第t个学习器的训练结果\n",
    "            X_2[:,t] = y_res_t\n",
    "        self.X_2_train_= X_2\n",
    "        return X_2\n",
    "    \n",
    "    def train_stack_2(\n",
    "        self\n",
    "        ,X\n",
    "        ,y\n",
    "    ):\n",
    "        '''\n",
    "        训练次级学习器\n",
    "        @return \n",
    "        '''\n",
    "        return self.models_stack_2_.fit(X,y)\n",
    "    \n",
    "    def fit(\n",
    "        self\n",
    "        , X\n",
    "        , y\n",
    "    ):\n",
    "        '''\n",
    "        先以初级学习器生成新的训练集\n",
    "        然后用次级学习器进行学习.\n",
    "        '''\n",
    "        self.train_stack_2(self.train_stack_1(X,y),y)\n",
    "        return self\n",
    "\n",
    "    def __stack1_X(self, X):\n",
    "        '''\n",
    "        用初级学习器生成新的训练集X_2\n",
    "        '''\n",
    "        # 样本数量\n",
    "        n = len(X)\n",
    "        # 生成的新的训练集X_2.shape = (n,T)\n",
    "        X_2 = np.empty(shape=(n, self.T_), dtype=np.int32)\n",
    "        # 初级学习器\n",
    "        models = self.models_stack_1_\n",
    "        for t in range(self.T_):\n",
    "            # 第t个学习器的训练结果\n",
    "            y_res = models[t].predict(X)\n",
    "            X_2[:,t] = y_res\n",
    "            \n",
    "        self.X_2_test_ = X_2\n",
    "        return X_2\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_2 = self.__stack1_X(X)\n",
    "        return self.models_stack_2_.predict(X_2)\n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        return np.sum(self.predict(X_test) == y_test) / len(y_test)\n",
    "        \n",
    "X,y = load_data()\n",
    "\n",
    "'''\n",
    "stack1_models = [\n",
    "    SVC(C=0.025)\n",
    "    ,LogisticRegression(C=0.01001,penalty='l1', max_iter=10)\n",
    "    ,GaussianNB(var_smoothing=0.000035)\n",
    "    ,MLPClassifier(hidden_layer_sizes=3)\n",
    "    ,DecisionTreeClassifier(max_depth=5)\n",
    "]\n",
    "'''\n",
    "\n",
    "stack1_models = [\n",
    "    SVC()\n",
    "    ,LogisticRegression()\n",
    "    ,GaussianNB()\n",
    "    ,MLPClassifier()\n",
    "    ,DecisionTreeClassifier()\n",
    "]\n",
    "\n",
    "stack2_model = DecisionTreeClassifier()\n",
    "\n",
    "X_train,X_test,y_train,y_test = load_data_split_standarlize()\n",
    "\n",
    "stack1_scores = []\n",
    "model_names = []\n",
    "X = standardize(X)\n",
    "\n",
    "for model in stack1_models:\n",
    "    model_names.append(model.__class__.__name__)\n",
    "    model.fit(X_train,y_train)\n",
    "    stack1_scores.append(model.score(X_test,y_test))\n",
    "\n",
    "print('stack1 mean scores : ', np.mean(stack1_scores))\n",
    "model_name_cor_score = dict(zip(model_names,stack1_scores))\n",
    "print('stack1 scores: ', model_name_cor_score)\n",
    "        \n",
    "print('-'*20 , 'Stacking score' , '-'*20)\n",
    "ss = SimpleStacking(stack1_models,stack2_model)\n",
    "ss.fit(X_train,y_train)\n",
    "print(ss.score(X_test,y_test))\n",
    "\n",
    "print('-' * 20, 'stack1_scores', '-'*20)\n",
    "for t in range(5):\n",
    "    print(score(y_test, ss.X_2_test_[:, t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVC': 0.7244444444444444, 'LogisticRegression': 0.7288888888888889, 'GaussianNB': 0.5644444444444444, 'MLPClassifier': 0.6777777777777778, 'DecisionTreeClassifier': 0.6044444444444445}\n",
      "**************************************************\n",
      "{'SVC': 0.7244444444444444, 'LogisticRegression': 0.7288888888888889, 'MLPClassifier': 0.6777777777777778}\n",
      "stack1 mean scores :  0.7103703703703704\n",
      "stack1 scores:  {'SVC': 0.7244444444444444, 'LogisticRegression': 0.7288888888888889, 'MLPClassifier': 0.6777777777777778}\n",
      "-------------------- Stacking score --------------------\n",
      "0.7844444444444445\n",
      "-------------------- stack1_scores --------------------\n",
      "0.5844444444444444\n",
      "0.64\n",
      "0.7444444444444445\n"
     ]
    }
   ],
   "source": [
    "X,y = load_data()\n",
    "\n",
    "stack1_models = [\n",
    "    SVC(C=0.025)\n",
    "    ,LogisticRegression(C=0.01001,penalty='l1', max_iter=10)\n",
    "    ,GaussianNB(var_smoothing=0.000035)\n",
    "    ,MLPClassifier(hidden_layer_sizes=3)\n",
    "    ,DecisionTreeClassifier(max_depth=5)\n",
    "]\n",
    "\n",
    "\n",
    "'''\n",
    "stack1_models = [\n",
    "    SVC()\n",
    "    ,LogisticRegression()\n",
    "    ,GaussianNB()\n",
    "    ,MLPClassifier()\n",
    "    ,DecisionTreeClassifier()\n",
    "]\n",
    "'''\n",
    "stack2_model = DecisionTreeClassifier()\n",
    "\n",
    "X_train,X_test,y_train,y_test = load_data_split_standarlize()\n",
    "\n",
    "stack1_scores = []\n",
    "model_names = []\n",
    "X = standardize(X)\n",
    "\n",
    "for model in stack1_models:\n",
    "    model_names.append(model.__class__.__name__)\n",
    "    model.fit(X_train,y_train)\n",
    "    stack1_scores.append(model.score(X_test,y_test))\n",
    "    \n",
    "print(dict(zip(model_names,stack1_scores)))\n",
    "print('*'*50)\n",
    "\n",
    "for m,s,n in zip(stack1_models,stack1_scores,model_names):\n",
    "    if s < 0.7:\n",
    "        stack1_models.remove(m)\n",
    "        stack1_scores.remove(s)\n",
    "        model_names.remove(n)\n",
    "    \n",
    "print(dict(zip(model_names,stack1_scores)))\n",
    "\n",
    "print('stack1 mean scores : ', np.mean(stack1_scores))\n",
    "model_name_cor_score = dict(zip(model_names,stack1_scores))\n",
    "print('stack1 scores: ', model_name_cor_score)\n",
    "        \n",
    "print('-'*20 , 'Stacking score' , '-'*20)\n",
    "ss = SimpleStacking(stack1_models,stack2_model)\n",
    "ss.fit(X_train,y_train)\n",
    "print(ss.score(X_test,y_test))\n",
    "\n",
    "print('-' * 20, 'stack1_scores', '-'*20)\n",
    "for t in range(len(stack1_models)):\n",
    "    print(score(y_test, ss.X_2_test_[:, t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 有不解之处,现在看起来stacking与基学习器的准确率有关系,结合以后效果更差了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可能写错了,需要引用一些更详尽的资料"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
