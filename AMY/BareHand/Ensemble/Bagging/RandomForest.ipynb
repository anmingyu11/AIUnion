{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林是Bagging的一个扩展变体,RF在以决策树为基学习器构建Bagging集成的基础上,进一步在决策树的训练过程中引入了随机属性选择.\n",
    "\n",
    "具体来说,传统决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性,而在RF中,对基决策树的每个结点,先从该结点的属性集合随机选择一个包含k个属性的子集,然后再从这个子集中选择一个人最优属性用于划分.这里的参数k控制了随机性的引入程度:\n",
    "\n",
    "若k=d则与传统决策树相同,若令 k=1 则是随机选择一个属性用于划分;一般情况下推荐值k=log2d\n",
    "\n",
    "随机森林中基学习器的多样性不仅来自样本扰动,还来自于属性扰动,这就使得最终集成泛化性能可通过个体学习器之间差异度的增加而进一步提升.\n",
    "\n",
    "随机森林的起始性能往往相对较差,特别是集成中只包含一个基学习器时."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78846154, 0.72099448, 0.78272981, 0.81792717, 0.77746479])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data():\n",
    "    data = load_digits()\n",
    "    return data.data,data.target\n",
    "\n",
    "X,y = load_data()\n",
    "cross_val_score(DecisionTreeClassifier(),X,y,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 2),\n",
       " (5, 2),\n",
       " (6, 2),\n",
       " (7, 2),\n",
       " (8, 3),\n",
       " (9, 3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def k_features(n_f):\n",
    "    return int(np.log2(n_f)) if n_f > 1 else n_f \n",
    "k = []\n",
    "for i in range(10):\n",
    "    k.append((i,k_features(i)))\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先,从统计的方面来看,由于学习任务的假设空间往往很大,可能有多个假设在训练集上达到同等性能,此时若使用单学习器可能因误选而导致泛化性能不佳,结合多个学习器则会减少这一风险;\n",
    "\n",
    "2. 从计算的方面来看,学习算法往往会陷入局部极小,有的局部极小点所对应的泛化性能可能很糟糕,而通过多次运行之后进行结合,有的局部极小点所对应的泛化性能可能很糟糕,而通过多次运行之后进行结合,可降低陷入局部极小点的风险;\n",
    "\n",
    "3. 从表示的方面来看,某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中,此时若使用单学习器则肯定无效,而通过结合多个学习器,由于相应的假设空间有所扩大,有可能学得更好的近似."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87087912, 0.80939227, 0.85793872, 0.90196078, 0.85633803])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=5)\n",
    "cross_val_score(rf,X,y,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=920786178, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=328534659, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=2082500840, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=10837842, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=752876524, splitter='best')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X,y)\n",
    "rf.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合方法\n",
    "\n",
    "### 对数值型输出\n",
    "- 平均法\n",
    "- 加权平均法\n",
    "\n",
    "## 对分类型输出\n",
    "- 绝对多数投票法\n",
    "- 相对多数投票法\n",
    "- 加权投票法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准RandomForest的子决策树的参数.\n",
    "\n",
    "```\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "                        max_features='auto', max_leaf_nodes=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, presort=False,\n",
    "                        random_state=813479198, splitter='best'),\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求和有更好的办法\n",
    "#cvs = []\n",
    "#for i in range(100):\n",
    "#    cvs.append(cross_val_score(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
    "#                        max_features='auto', max_leaf_nodes=None,\n",
    "#                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                        min_samples_leaf=1, min_samples_split=2,\n",
    "#                        min_weight_fraction_leaf=0.0, presort=False,splitter='best'),X,y,cv=5).mean())\n",
    "#np.mean(cvs)# 0.6639091719045126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my test score :  0.9555555555555556\n",
      "sklearn rf test score :  0.9466666666666667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "my oob score:  0.9272457312546399\n",
      "sklearn rf oob score 0.9360044518642181\n"
     ]
    }
   ],
   "source": [
    "class RandomForest:\n",
    "    \n",
    "    def __init__(\n",
    "        self\n",
    "        ,T = 30\n",
    "    ):\n",
    "        '''\n",
    "        @params T 循环的次数\n",
    "        '''\n",
    "        self.T_ = T\n",
    "        self.estimators_ = None\n",
    "        self.X_ = None\n",
    "        self.y_ = None\n",
    "        self.es_inbag_sample_args_ = None\n",
    "        self.es_oobag_sample_args_ = None\n",
    "        self.es_params_ = {\n",
    "            'class_weight':None\n",
    "            ,'criterion':'gini'\n",
    "            ,'max_depth':6\n",
    "            ,'max_leaf_nodes':None\n",
    "            ,'max_features':'auto'\n",
    "            ,'min_impurity_decrease':0.0\n",
    "            ,'min_impurity_split':None\n",
    "            ,'min_samples_leaf':1\n",
    "            ,'min_samples_split':2\n",
    "            ,'min_weight_fraction_leaf':0.0\n",
    "            ,'presort':False\n",
    "            ,'splitter':'best'\n",
    "        }#0.6639091719045126\n",
    "        \n",
    "    def bootstrap_sampling(self,X,y):\n",
    "        '''\n",
    "        自助采样法\n",
    "        '''\n",
    "        n = len(y)\n",
    "        args = np.arange(0,n,1)\n",
    "        # 放回抽样\n",
    "        samp_args = np.random.choice(args, n, True)\n",
    "        uniq_args = np.unique(samp_args) \n",
    "        return X[uniq_args] ,y[uniq_args] ,uniq_args\n",
    "\n",
    "    def __oob_sample_args(self,n,inb_sample_args):\n",
    "        '''\n",
    "        获取包外样本索引\n",
    "        '''\n",
    "        args = np.arange(0 ,n ,1)\n",
    "        args[inb_sample_args] = -1\n",
    "        return args[ args >= 0 ]\n",
    "        \n",
    "    def fit(self ,X ,y):\n",
    "        # 初始化成员变量\n",
    "        n = len(y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.estimators_ = []\n",
    "        self.es_inbag_sample_args_ = []\n",
    "        self.es_oobag_sample_args_ = []\n",
    "        dt_params = self.es_params_\n",
    "        for i in range(self.T_):\n",
    "            # bootstrap采样\n",
    "            X_sample,y_sample,sample_args = self.bootstrap_sampling(X,y)\n",
    "            # print(sample_args.shape)\n",
    "            # 用采样数据训练数据\n",
    "            rnd_seed = np.random.randint(2**32 - 1)\n",
    "            clf = DecisionTreeClassifier(\n",
    "                class_weight=dt_params['class_weight']\n",
    "                ,criterion=dt_params['criterion']\n",
    "                ,max_depth=dt_params['max_depth']\n",
    "                ,max_leaf_nodes=dt_params['max_leaf_nodes']\n",
    "                ,max_features=dt_params['max_features']\n",
    "                ,min_impurity_decrease=dt_params['min_impurity_decrease']\n",
    "                ,min_impurity_split=dt_params['min_impurity_split']\n",
    "                ,min_samples_leaf=dt_params['min_samples_leaf']\n",
    "                ,min_samples_split=dt_params['min_samples_split']\n",
    "                ,min_weight_fraction_leaf=dt_params['min_weight_fraction_leaf']\n",
    "                ,presort=dt_params['presort']\n",
    "                ,splitter=dt_params['splitter']\n",
    "                , random_state=rnd_seed\n",
    "            ).fit(X_sample,y_sample)\n",
    "            # 更新成员变量\n",
    "            self.es_inbag_sample_args_.append(sample_args)\n",
    "            self.es_oobag_sample_args_.append(self.__oob_sample_args(n, sample_args))\n",
    "            self.estimators_.append(clf)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        n = len(X_test)\n",
    "        y_p_set = np.zeros((self.T_,n),dtype=np.int32)\n",
    "        # For vote\n",
    "        for i , est in enumerate(self.estimators_):\n",
    "            yp = est.predict(X_test)\n",
    "            y_p_set[i]=yp\n",
    "        # 初始化预测结果\n",
    "        y_res = np.zeros(n, dtype=np.int32)\n",
    "        for i in range(n):\n",
    "            # 对每个样本各个预测器的投票结果\n",
    "            uniqs, uniq_cnts = np.unique(y_p_set[:,i], return_counts=True)\n",
    "            # 获取投票数量最大的索引\n",
    "            cnt_argmax = np.argmax(uniq_cnts)\n",
    "            #print('uniqs : ',uniqs, 'cnts : ',uniq_cnts,'argmax:',cnt_argmax)\n",
    "            y_res[i] = uniqs[cnt_argmax]\n",
    "        return y_res\n",
    "    \n",
    "    def __oob_predict(self):\n",
    "        # 仅考虑未使用x的分类器进行预测.\n",
    "        \n",
    "        # 投票的稀疏矩阵,行对应m个样本,列对应投票结果\n",
    "        y_res = []\n",
    "        # 用于预测的袋外数据\n",
    "        y_args = []\n",
    "        for i , x in enumerate(self.X_):\n",
    "            # 第i个样本的投票结果\n",
    "            y_i = []\n",
    "            # 遍历 T个基分类器\n",
    "            for t in range(self.T_):\n",
    "                # 第t个分类器的袋内数据索引.\n",
    "                es_inbag_args = self.es_inbag_sample_args_[t]\n",
    "                # 判断当前数据x是否参与了分类器t的训练.\n",
    "                if np.any(es_inbag_args == i):\n",
    "                    # 如果已经参与就终结循环,遍历后面的分类器.\n",
    "                    continue\n",
    "                # 当前分类没有用到当前数据x = X[i],用当前分类器来预测数据x.\n",
    "                y_p = self.estimators_[t].predict(x.reshape(1,-1))\n",
    "                # 记录投票结果\n",
    "                y_i.append(y_p.ravel()[0])\n",
    "            # 没有任何分类器没用到当前数据\n",
    "            if len(y_i) == 0:\n",
    "                continue\n",
    "            y_res.append(y_i)\n",
    "            y_args.append(i)\n",
    "        n = len(y_args)\n",
    "        y_final_p = np.full(n, -1)\n",
    "        \n",
    "        for i in range(n):\n",
    "            # 对每个样本各个预测器的投票结果\n",
    "            uniqs, uniq_cnts = np.unique(np.array(y_res[i]), return_counts=True)\n",
    "            # 获取投票数量最大的索引\n",
    "            cnt_argmax = np.argmax(uniq_cnts)\n",
    "            # print('uniqs : ',uniqs, 'cnts : ',uniq_cnts,'argmax:',cnt_argmax)\n",
    "            y_final_p[i] = uniqs[cnt_argmax]\n",
    "        return y_final_p, y_args\n",
    "    \n",
    "    def oob_score(self):\n",
    "        y_p , y_args = self.__oob_predict()\n",
    "        return np.sum(self.y_[y_args] == y_p) / len(y_args)\n",
    "    \n",
    "X,y = load_data()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "T=50\n",
    "# sklearn学习器的训练\n",
    "sk_rf_clf = RandomForestClassifier(\n",
    "    n_estimators=T\n",
    "    , class_weight=None\n",
    "    , criterion='gini'\n",
    "    , max_depth=6\n",
    "    , max_features='auto'\n",
    "    , max_leaf_nodes=None\n",
    "    , min_impurity_decrease=0.0\n",
    "    , min_impurity_split=None\n",
    "    , min_samples_leaf=1\n",
    "    , min_samples_split=2\n",
    "    , min_weight_fraction_leaf=0.0\n",
    ").fit(X_train,y_train)\n",
    "sk_rf_score = sk_rf_clf.score(X_test,y_test)\n",
    "# 我的Simple Bagging 训练\n",
    "sb = RandomForest(T=T).fit(X_train,y_train)\n",
    "# 留出法\n",
    "print('my test score : ' , np.sum(sb.predict(X_test)==y_test) / len(y_test))\n",
    "print('sklearn rf test score : ' , sk_rf_score)\n",
    "\n",
    "print(\"-\"*100)\n",
    "# 自采样oob\n",
    "sk_rf_clf = RandomForestClassifier(\n",
    "    n_estimators=T\n",
    "    , class_weight=None\n",
    "    , criterion='gini'\n",
    "    , max_depth=6\n",
    "    , max_features='auto'\n",
    "    , max_leaf_nodes=None\n",
    "    , min_impurity_decrease=0.0\n",
    "    , min_impurity_split=None\n",
    "    , min_samples_leaf=1\n",
    "    , min_samples_split=2\n",
    "    , min_weight_fraction_leaf=0.0\n",
    "    , oob_score=True\n",
    ").fit(X,y)\n",
    "print('my oob score: ', sb.oob_score())\n",
    "print('sklearn rf oob score' , sk_rf_clf.oob_score_)\n",
    "\n",
    "#sb.oob_predict()\n",
    "#np.argpartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT mean scores :  0.7186666666666668\n"
     ]
    }
   ],
   "source": [
    "X,y = load_data()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "dt_params = {\n",
    "    'class_weight':None\n",
    "    ,'criterion':'gini'\n",
    "    ,'max_depth':6\n",
    "    ,'max_leaf_nodes':None\n",
    "    ,'max_features':'auto'\n",
    "    ,'min_impurity_decrease':0.0\n",
    "    ,'min_impurity_split':None\n",
    "    ,'min_samples_leaf':1\n",
    "    ,'min_samples_split':2\n",
    "    ,'min_weight_fraction_leaf':0.0\n",
    "    ,'presort':False\n",
    "    ,'splitter':'best'\n",
    "}\n",
    "dt_scores=[]\n",
    "for i in range(50):\n",
    "    dt_clf = DecisionTreeClassifier(\n",
    "                    class_weight=dt_params['class_weight']\n",
    "                    ,criterion=dt_params['criterion']\n",
    "                    ,max_depth=dt_params['max_depth']\n",
    "                    ,max_leaf_nodes=dt_params['max_leaf_nodes']\n",
    "                    ,max_features=dt_params['max_features']\n",
    "                    ,min_impurity_decrease=dt_params['min_impurity_decrease']\n",
    "                    ,min_impurity_split=dt_params['min_impurity_split']\n",
    "                    ,min_samples_leaf=dt_params['min_samples_leaf']\n",
    "                    ,min_samples_split=dt_params['min_samples_split']\n",
    "                    ,min_weight_fraction_leaf=dt_params['min_weight_fraction_leaf']\n",
    "                    ,presort=dt_params['presort']\n",
    "                    ,splitter=dt_params['splitter']\n",
    "                )\n",
    "    dt_clf.fit(X_train,y_train)\n",
    "    dt_scores.append(dt_clf.score(X_test,y_test))\n",
    "print('DT mean scores : ',np.mean(dt_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
